{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion Week 3: PySpark Review (Richard's Section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Comprehensions, Lambdas, Generators, and Yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List comprehensions:\n",
    "x = range(10)\n",
    "\n",
    "# y = [n**2 for n in x if n < 5]\n",
    "\n",
    "print y\n",
    "\n",
    "y2 = [n**2 if n < 5 else 0 for n in x ]\n",
    "print y2\n",
    "\n",
    "# print [a * b for a in y for b in y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lambda Expressions\n",
    "\n",
    "def convert_me(n):\n",
    "    return 1./ n ** 2\n",
    "\n",
    "convert_you = lambda x: 1./x ** 2\n",
    "\n",
    "convert_me(10) == convert_you(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen1 = lambda n: (i for i in range(n))\n",
    "\n",
    "# def gen2(n):\n",
    "#     for i in range(n):\n",
    "#         yield i\n",
    "        \n",
    "x = gen1(5)\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print next(x)\n",
    "print next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g1 = gen1(5)\n",
    "g2 = gen2(5)\n",
    "for i in range(5):\n",
    "    print next(g1) == next(g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "\n",
    "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load new RDD using a collection from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([\"one\", \"two\", \"three\"], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rdd.collect()` return a list that contains all of the elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_partitions(rdd1)\n",
    "# rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of making our own collection, let's load in a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd2 = sc.textFile('link_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the first few entries in this document - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Here is the Raw document\"\n",
    "\n",
    "!head -n 5 link_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something interesting with this data - get the domains of all of the websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_site(iterator):\n",
    "    for link in iterator:\n",
    "        index = link.find(\"www.\")\n",
    "        end = link.find(\".com\")\n",
    "        if index > 0 and end > 0:\n",
    "            yield link[index + 4: end]\n",
    "\n",
    "site_rdd = rdd2.mapPartitions(lambda iterator: get_site(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siterddcollect = site_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siterddcollect[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print rdd2.getNumPartitions(), rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice how the object itself is not very eventful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site_rdd.distinct().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the raw implementation of `rdd.distinct()` from PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distinct(self, numPartitions=2):\n",
    "    \"\"\"\n",
    "    Return a new RDD containing the distinct elements in this RDD.\n",
    "\n",
    "    >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
    "    [1, 2, 3]\n",
    "    \"\"\"\n",
    "    return self.map(lambda key: (key, None)) \\\n",
    "               .reduceByKey(lambda value, val2: 1000, numPartitions) \\\n",
    "               .map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get the distinct URLs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# distinct(site_rdd).take(20)\n",
    "# site_rdd.distinct().take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd1.map(lambda x: (1, x)).reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "## Back to EXTERNAL HASHING. \n",
    "\n",
    "Recall the second lecture where we talked about partitioning your data:\n",
    "\n",
    "<img src=https://docs.oracle.com/cd/B28359_01/server.111/b32024/img/cncpt158.gif />\n",
    "\n",
    "### Ok well let's do this in PySpark (very important for Part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import string\n",
    "# print string.lowercase\n",
    "# print string.lowercase[:5]\n",
    "\n",
    "def check_partitions(rdd):\n",
    "    def p(n, iterate):\n",
    "        for i in iterate:\n",
    "            yield(n, i)\n",
    "    return rdd.mapPartitionsWithIndex(p).collect()\n",
    "\n",
    "# sc = n # just bc it's more intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first motivate the problem of having a hash function with non-uniform data in a discrete setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% pylab inline\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "samples = np.random.poisson(5, 1000)\n",
    "# samples = np.random.rayleigh(3, 100)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(samples)\n",
    "plt.xlabel(\"Elements\")\n",
    "plt.figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make a new RDD..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(4), 4)\n",
    "print check_partitions(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a buncha random values for e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_random(iterator):\n",
    "    return ((np.random.rayleigh(200), i) for i in np.random.poisson(4, 20))\n",
    "\n",
    "random_rdd = rdd.mapPartitions(lambda iterator: generate_random(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in check_partitions(random_rdd)[:20]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partitioned = random_rdd.partitionBy(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in sorted(check_partitions(partitioned)):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "partition_distribution = partitioned\\\n",
    "            .mapPartitionsWithIndex(lambda k, y: (k for _ in y))\\\n",
    "            .collect() \n",
    "        \n",
    "        \n",
    "# print Counter(partition_distribution)\n",
    "plt.hist(partition_distribution, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As shown above, partitioning is done (albeit skewed)! But what's the point of partitioning again...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you think of a better partition function to fix our skew?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def partition_better(key):\n",
    "    pass\n",
    "\n",
    "goodrdd = random_rdd.partitionBy(10, partitionFunc=partition_better)\n",
    "partition_distribution = goodrdd\\\n",
    "            .mapPartitionsWithIndex(lambda y, iter: (y for i in iter))\\\n",
    "            .collect() \n",
    "        \n",
    "        \n",
    "print Counter(partition_distribution)\n",
    "plt.hist(partition_distribution, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
